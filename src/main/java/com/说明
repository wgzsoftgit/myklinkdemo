Flink使用 DataSet 和 DataStream 代表数据集。DateSet 用于批处理，代表数据是有限的，
而 DataStream 用于流数据，代表数据是无界的。数据集中的数据是不可以变的，也就是说不能对其中的元素增加或删除。
我们通过数据源创建 DataSet 或者 DataStream ，通过 map，filter 等转换（transform）操作对数据集进行操作产生新的数据集
批处理：
DataSet API，对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便的采用 Flink 提供的各种操作符对分布式数据集进行各种操作。
DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便的采用 Flink 提供的各种操作符对分布式数据流进行各种操作。
Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过 Flink 提供的类 SQL 的 DSL 对关系表进行各种查询操作。

作者：Flink_China
链接：https://juejin.im/post/5e660d32e51d4526e32c3f70
来源：掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
DataSet<String> text = env.readTextFile("file:///D:\\words.txt");
text.print();
流处理：


StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
DataStream<String> text = env.readTextFile("file:///D:\\words.txt");
text.print();
env.execute();


链接：https://www.jianshu.com/p/6c0f20660c63


启动
进入bin目录：cd /root/kingyifan/flink/flink-1.7.2/bin/
启动flink：./start-cluster.sh
停止的话，使用stop-cluster.sh即可
阿里云服务器需要
进入配置iptables：vi /etc/sysconfig/iptables
增加：-A INPUT -m state --state NEW -m tcp -p tcp --dport 8081 -j ACCEPT
重启防火墙：service iptables restart
访问ui界面：192.168.241.134:8081

启动一个终端（端口随意）：nc -lk 8888      //输入数据
启动二个终端（进入bin目录：cd /root/kingyifan/flink/flink-1.7.2/
命令：bin/flink run examples/streaming/SocketWindowWordCount.jar --port 8888
打开第三个会话：
进入log目录：cd /root/kingyifan/flink/flink-1.7.2/log
命令：tail -f flink-root-taskexecutor-5-localhost.localdomain.out（不一定是这个日志文件）



env.fromElements("Please count", "the words", "but not this");     、、 DataSet



1.基于文件
env.readTextFile("file:///path/to/file"); 读取文件内容 逐行读取文件中的行并返回类型的数据集的方法String
读取文本文件，文件遵循TextInputFormat 读取规则，逐行读取并返回。
//flink还支持CSV文件，但在这种情况下，它不会返回字符串数据集。它将尝试解析每一行并返回Tuple实例的数据集：
DataSet<Tuple2<Long, String>> lines = env.readCsvFile("data.csv")
//将尝试读取本地文件。如果要从HDFS读取文件，则需要指定hdfs://协议
env.readCsvFile("hdfs:///path/to/file.txt")
2.基于插座     从socket中读取数据，元素可以通过一个分隔符切开。
env.socketTextStream("192.168.220.128", port, "\n");操作服务器实时的数据流  DataStream
3.基于集合
DataSet<String> letters = env.fromCollection(Arrays.asList("a", "b", "c"));  、、DataSet
通过java 的collection集合创建一个数据流，集合中的所有元素必须是相同类型的。
4.自定义输入
env.addSource(new WikipediaEditsSource())读取wiki ipc 日志的资源  数据源  可以实现读取第三方数据源的数据
系统内置提供了一批connectors，连接器会提供对应的source支持【kafka

现在到数据处理部分！如何实现处理数据的算法？为此，您可以使用许多类似于Java 8流操作的操作，例如
map - 使用用户定义的函数转换数据集中的项目。每个输入元素都转换为一个输出元素
filter - 根据用户定义的函数过滤数据集中的项目
flatMap - 类似于map运算符，但允许返回零个，一个或多个元素
groupBy - 按键分组元素。类似于GROUP BYSQL中的运算符
project - 选择元组数据集中的指定字段，类似于SELECTSQL中的运算符
reduce - 使用用户定义的函数将数据集中的元素组合成单个值
Java流与这些操作之间的最大区别在于，Java 8可以处理内存中的数据并可以访问本地数据，而Flink可以处理分布式环境中群集上的数据。
                .types(Long.class, String.class);
                
要将数据写入文件，我们需要使用类中的writeAsText方法DataSet：
DataSet<Integer> ds = ...
 
ds.writeAsText("path/to/file"); 等方法将数据输出到其他介质


Flink的常用转换算子
1.3.1 Map和Filter（刚刚演示过了）
1.3.2 flatMap，keyBy，sum，union（和Spark是基本一样的）
1.3.3 connect，MapFunction和coMapFunction

开发的时候容易出错，如果每次都打包进行调试，比较麻烦，并且也不好定位问题，可以在scala shell命令行下进行调试
bin/start-scala-shell.sh [local|remote|yarn] [options] <args>
[root@node1 bin]# ./start-scala-shell.sh local
相信你也有可能出现我现在的情况，此时报错“无法创建DispatcherResourceManagerComponent”

此时如果要修正这个问题，我们可以cd /usr/local/flink-1.10.0/conf，然后把添加一个这样的参数
rest.port:8085
把端口给修改了之后，就可以成功跑起来啦
当然这个东西其实并不太重要，因为Flink-shell远远不及Spark-shell好用


了解一个实时的程序，主要我们需要去了解3个方面，
数据源，数据的处理及数据的输出

